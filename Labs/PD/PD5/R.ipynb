{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Causal Inference with Randomized Experiments\n",
    "\n",
    "As we saw in class, the problem of causal inference is the fact that we cannot observe, for the same individual, the outcome when they are put under a treatment and the same outcome when they are not put under the treatment. This means that we cannot find the individual effect of the treatment, which is the difference of the outcomes in either circumstance. However, we can approximate the expected difference at the population level, the **average treatment effect**. We also touched upon the fact that we cannot do this easily with observational data, as our observations may be biased because of underlying relationships between the real treatment and unobserved variables, meaning that a simple regression of the outcome on the treatment will not render a faithful approximation of the real. The following example will help illustrate how this failure to consider selection bias can lead us to both type 1 and type 2 errors.\n",
    "\n",
    "## Type 1 error:\n",
    "\n",
    "Imagine we are researchers in the first phase of research for a new drug aimed at reducing blood cholesterol levels. Because this is the first phase, we would like to assess the safety of the treatment. Therefor, we want to quantify the effect the the regime has on lung health. Let $Y(d)$ be the potential outcome of an indicator of overall respiratory health. Let $D$ be the observed treatment variable, and assume first that treatment was not assigned randomly, but that it is correlated with underlying variables lifestyle and demographic characteristics. Formally, we have the following model:\n",
    "\n",
    "\\begin{gather*}\n",
    "Y(0) = \\theta_0+\\epsilon_0\\\\\n",
    "Y(1) = \\theta_1+\\epsilon_1\\\\\n",
    "D = \\mathbb{1}(\\nu>0)\\\\\n",
    "Y=Y(D)=(1-D)Y(0)+DY(1)\n",
    "\\end{gather*}\n",
    "\n",
    "Here. $\\theta_0$ and $\\theta_1$ are the averages of their respective potential outcomes, $(\\epsilon_0, \\epsilon_1, \\nu)$ is a random vector of variables that are jointly distributed with mean 0 and covariance $\\Sigma$. $\\epsilon_0$ and $\\epsilon_1$ are determinants of overall lung health like lifestyle, exposure to environmental contaminants, diet, etc. For this specific example, the variable that determines treatment, $\\nu$, is negativelly correlated with $\\epsilon_0$ and $\\epsilon_1$. One way to interpret this is that people with worse overall lifestyle patterns that provoke worse lung health may also tend to have worse blood cholesterol content, and therefor are more likely to seek the treatment in question.\n",
    "\n",
    "For simulating the data, we will generate 100 000 observations of the variables in our model. We will assume that the average potential outcomes are $\\theta_0 = \\theta_1 =1$ and the covariance matrix is\n",
    "\n",
    "$$\n",
    "\\Sigma = \\left(\\begin{matrix}\n",
    "1 & 0 & -0.5\\\\\n",
    "0 & 1 & -0.5\\\\\n",
    "-0.5 & -0.5 & 1\n",
    "\\end{matrix}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by simulating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(MASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "N <- 100000\n",
    "cov_matrix <- matrix(c(1, 0, -.5, 0, 1, -.5, -.5, -.5, 1), nrow = 3)\n",
    "random_shocks <- mvrnorm(N, c(0, 0, 0), cov_matrix)\n",
    "epsilon0 <- random_shocks[, 1]\n",
    "epsilon1 <- random_shocks[, 2]\n",
    "nu <- random_shocks[, 3] > 0\n",
    "theta0 <- 1\n",
    "theta1 <- 1\n",
    "Y_0 <- theta0 + epsilon0\n",
    "Y_1 <- theta1 + epsilon1\n",
    "D_biased <- nu > 0\n",
    "Y_biased <- (1 - D_biased) * Y_0 + D_biased * Y_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the average treatment effect is defined as\n",
    "\n",
    "$$\n",
    "\\delta=E[Y(1) - Y(0)] = E[Y(1)] - E[Y(0)]\n",
    "$$\n",
    "\n",
    "In our example\n",
    "\n",
    "$$\n",
    "\\delta = \\theta_1-\\theta_0\n",
    "$$\n",
    "\n",
    "Given the way we have defined our model, this should just be 0. In reality, we cannot observe both $Y(0)$ and $Y(1)$. However, we can play by our own rules when we are the ones simulating the data, and find the actual ATE. We should expect to see a very small number as the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "-0.00586655718340623"
      ],
      "text/latex": [
       "-0.00586655718340623"
      ],
      "text/markdown": [
       "-0.00586655718340623"
      ],
      "text/plain": [
       "[1] -0.005866557"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(Y_1) - mean(Y_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the variance of the individual diferences and see that it is much larger than the average diference. This should hint that the divergence from the true value of zero comes from imprecission caused by the random vairiables $\\epsilon_d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.00446989828175887"
      ],
      "text/latex": [
       "0.00446989828175887"
      ],
      "text/markdown": [
       "0.00446989828175887"
      ],
      "text/plain": [
       "[1] 0.004469898"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sd(Y_1 - Y_0) / sqrt(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we would approximate $\\theta_0, \\theta_1$ by getting each group's mean. This is:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_d = \\frac{\\sum_{\\{i\\in[1,...,n]:D_i=d\\}}Y_i}{n_d} = \\frac{\\mathbb{E}_n[Y1(D=d)]}{\\mathbb{E}_n[1(D=d)]}\n",
    "$$\n",
    "\n",
    "Then we can approximate the ATE with $\\hat{\\theta}_1-\\hat{\\theta}_0$. We can also get confidence intervals for these estimators\n",
    "\n",
    "$$\n",
    "Var(\\hat{\\theta}_1)=\\frac{Var(Y|D=d)}{nP(D = d)}\n",
    "$$\n",
    "\n",
    "With this, our $\\hat{\\delta}$ estimate is distributed as:\n",
    "\n",
    "$$\n",
    "\\hat{\\delta}\\sim_a\\mathcal{N}(\\delta, Var(\\hat{\\theta}_1) + Var(\\hat{\\theta}_2))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "-0.808259131297483"
      ],
      "text/latex": [
       "-0.808259131297483"
      ],
      "text/markdown": [
       "-0.808259131297483"
      ],
      "text/plain": [
       "[1] -0.8082591"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(Y_biased * D_biased) / mean(D_biased) - mean(Y_biased * (1 - D_biased)) / mean(1 - D_biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.00409901664093723"
      ],
      "text/latex": [
       "0.00409901664093723"
      ],
      "text/markdown": [
       "0.00409901664093723"
      ],
      "text/plain": [
       "[1] 0.004099017"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ate_variance <- var(Y_biased[D_biased]) / mean(D_biased) / N + var(Y_biased[1 - D_biased]) / mean(1 - D_biased) / N\n",
    "sqrt(ate_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very different result from the one above. First of all, we get a result that is relatively large in comparison to the real value of zero. Also, we get a very precise result that would imply high significance of our variable. This would lead us to reject the null hypothesis, even though it is true. For more clarity, we can run a regression of the recentered variables variables. Recentering is simply subtracting the mean.\n",
    "\n",
    "We see the same results, plus a very large $t$ statistic and a very small $p$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Y ~ 0 + D, data = biased_data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.0622 -0.6146  0.0026  0.6169  3.6934 \n",
       "\n",
       "Coefficients:\n",
       "  Estimate Std. Error t value Pr(>|t|)    \n",
       "D -0.80826    0.00578  -139.8   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 0.9139 on 99999 degrees of freedom\n",
       "Multiple R-squared:  0.1636,\tAdjusted R-squared:  0.1636 \n",
       "F-statistic: 1.956e+04 on 1 and 99999 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "biased_data <- data.frame(Y = Y_biased - mean(Y_biased), D = D_biased - mean(D_biased))\n",
    "summary(lm(Y ~ 0 + D, biased_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because selection bias leads to an apparent negative relationship between the treatment and the outcome. In our research setting, we would interpret this finding as the drug having a negative effect on lung health, even though the true ATE is of 0. This relationship only arises because of underlying variables that determine both $Y(d)$ and $D$ simultaneously. A variable that affects both the propensity of treatment and the outcome is called a **confounder**, and not taking confounders into account can lead to the kinds of issues we are seeing in this example. One way of tackling the issue of confounding is to use randomized treatments. Now in our research example, instead of having a $D$ treatment that is correlated with some unobservables, we randomly assign the treatment to the population of interest. We can simulate that through our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "D_random <- rnorm(N) > 0\n",
    "Y_random <- (1 - D_random) * Y_0 + D_random * Y_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now estimate the ATE and its variance like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "-0.00670904939968975"
      ],
      "text/latex": [
       "-0.00670904939968975"
      ],
      "text/markdown": [
       "-0.00670904939968975"
      ],
      "text/plain": [
       "[1] -0.006709049"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(Y_random * D_random) / mean(D_random) - mean(Y_random * (1 - D_random)) / mean(1 - D_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.00446297893145206"
      ],
      "text/latex": [
       "0.00446297893145206"
      ],
      "text/markdown": [
       "0.00446297893145206"
      ],
      "text/plain": [
       "[1] 0.004462979"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ate_variance <- var(Y_random[D_random]) / mean(D_random) / N + var(Y_random[1 - D_random]) / mean(1 - D_random) / N\n",
    "sqrt(ate_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that we get a much smaller value for the treatment effect, and that it is not even significant at the standard levels. To see this, we can conduct a regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Y ~ 0 + D, data = random_data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.4277 -0.6736 -0.0007  0.6789  4.3606 \n",
       "\n",
       "Coefficients:\n",
       "   Estimate Std. Error t value Pr(>|t|)\n",
       "D -0.006709   0.006331   -1.06    0.289\n",
       "\n",
       "Residual standard error: 1.001 on 99999 degrees of freedom\n",
       "Multiple R-squared:  1.123e-05,\tAdjusted R-squared:  1.231e-06 \n",
       "F-statistic: 1.123 on 1 and 99999 DF,  p-value: 0.2892\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_data <- data.frame(Y = Y_random - mean(Y_random), D = D_random - mean(D_random))\n",
    "summary(lm(Y ~ 0 + D, random_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The great utility of randomization is that it ensures that the realized treatment and the potential outcomes are uncorrelated, which leads to unbiased estimation.\n",
    "\n",
    "## Type 2 error\n",
    "\n",
    "Now, after having made sure that our treatment is safe, we can now look into its effectiveness in reducing blood cholesterol. Let the outcome now be inversely proportional to the blood cholesterol measure, such that a higher value represents lower cholesterol levels. For it to be considered effective it would have to have a positive and significant ATE. We will set the ATE for this example to 0.8. We will start with the observational setting, where we do not randomize the treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "theta0 <- 1\n",
    "theta1 <- 1.8\n",
    "Y_0 <- theta0 + epsilon0\n",
    "Y_1 <- theta1 + epsilon1\n",
    "D_biased <- nu > 0\n",
    "Y_biased <- (1 - D_biased) * Y_0 + D_biased * Y_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run a regression for the demeaned variables, we see that we get a small and insignificant estimator for our treatment effect. Again, the correlation between the treatment and the potential outcomes is causing us to get results that do not reflect reality. In this case we are led to not reject the null hypothesis, even though it is false and the true ATE is quite different from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Y ~ 0 + D, data = biased_data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.0622 -0.6146  0.0026  0.6169  3.6934 \n",
       "\n",
       "Coefficients:\n",
       "   Estimate Std. Error t value Pr(>|t|)\n",
       "D -0.008259   0.005780  -1.429    0.153\n",
       "\n",
       "Residual standard error: 0.9139 on 99999 degrees of freedom\n",
       "Multiple R-squared:  2.042e-05,\tAdjusted R-squared:  1.042e-05 \n",
       "F-statistic: 2.042 on 1 and 99999 DF,  p-value: 0.153\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "biased_data <- data.frame(Y = Y_biased - mean(Y_biased), D = D_biased - mean(D_biased))\n",
    "summary(lm(Y ~ 0 + D, biased_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with random assignment, we get much closer to the true ATE, and we can now (correctly) reject the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "D_random <- rnorm(N) > 0\n",
    "Y_random <- (1 - D_random) * Y_0 + D_random * Y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Y ~ 0 + D, data = random_data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.4628 -0.6752  0.0016  0.6799  4.3629 \n",
       "\n",
       "Coefficients:\n",
       "  Estimate Std. Error t value Pr(>|t|)    \n",
       "D 0.794609   0.006326   125.6   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 1 on 99999 degrees of freedom\n",
       "Multiple R-squared:  0.1363,\tAdjusted R-squared:  0.1363 \n",
       "F-statistic: 1.578e+04 on 1 and 99999 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_data <- data.frame(Y = Y_random - mean(Y_random), D = D_random - mean(D_random))\n",
    "summary(lm(Y ~ 0 + D, random_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification through strong ignorability:\n",
    "\n",
    "Sometimes we don't have the means to conduct a full RCT. Most of the time we only have data available from surveys and administrative records. The root of the estimation bias is solved by randomization, but the lack of randomization is not the cause of the bias itself. We can find other procedures that can lead us to an unbiased estimate even with a biased treatment. Recall from class that we can identify treatment effects through strong ignorability. We call $D_i$ strongly ignorable conditional on a vector $X$ if it follows:\n",
    "\n",
    "- Ignorability: For a given value of $X_i$, $D_i$ is unrelated to the potential outcomes\n",
    "- Positivity: $P(D_i=1|X_i)\\in(0, 1)$. For a given value of $X_i$, there are treated and untreated individuals.\n",
    "\n",
    "The second condition is somewhat trivial but necessary to make sure that at any value of $X_i$ there exists a split, even if it is very unbalanced. The first condition, on the other hand, hearkens back to a topic we have explored before. Recall the partialling out procedure, where, for a variable $V_i$, we would construct $\\tilde{V}_i$ such that\n",
    "\n",
    "$$\n",
    "\\tilde{V}_i=V_i-E[V_i|X_i]\n",
    "$$\n",
    "\n",
    "We refer back to partialling out in this situation because, through it, we are able to \"remove\" the influence of $X$ (in this case, our $\\epsilon_0$ and $\\epsilon_1$ variables) on $D$ and on $Y$. Therefor, we would expect the regression between the variables $\\tilde{Y}$ and $\\tilde{D}$ to give us a better estimate even if the base variables are biased. Note that this is because we can observe $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Y ~ 0 + D, data = resid_data)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-6.2670  0.9131  1.3979  1.8893 18.0051 \n",
       "\n",
       "Coefficients:\n",
       "  Estimate Std. Error t value Pr(>|t|)    \n",
       "D  0.09169    0.00173      53   <2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 1.58 on 99999 degrees of freedom\n",
       "Multiple R-squared:  0.02733,\tAdjusted R-squared:  0.02732 \n",
       "F-statistic:  2809 on 1 and 99999 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D_data <- data.frame(D = D_biased, eps0 = epsilon0, eps1 = epsilon1)\n",
    "D_resid <- glm(D ~ 0 + eps0 + eps1, \"binomial\", D_data)$residuals\n",
    "Y_data <- data.frame(Y = Y_biased, eps0 = epsilon0, eps1 = epsilon1)\n",
    "Y_resid <- lm(Y ~ 0 + eps0 + eps1, Y_data)$residuals\n",
    "resid_data <- data.frame(D = D_resid, Y = Y_resid)\n",
    "summary(lm(Y ~ 0 + D, resid_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
